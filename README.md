# avito_test_task
Восстановление пропущенных пробелов в тексте с помощью NLP / DL / алгоритма.

# Подход к решению задачи сегментации слов

Этот проект решает задачу сегментации слов, где требуется восстановить пробелы в текстах без пробелов (например, `"Helloworld"` → `"Hello world"`) с использованием модели `google/byt5-small`. Решение реализовано в Kaggle-ноутбуке с ускорителем GPU T4 x2. Ниже описан подход, включая этапы подготовки данных, обучения модели, инференса и обработки результатов.

## 1. Обзор задачи
Задача сегментации слов заключается в восстановлении пробелов в текстах, где слова слиты вместе. Например:
- Вход: `"Helloworld"`, `"СмартфонКнига"`.
- Выход: `"Hello world"`, `"Смартфон Книга"`.
- Формат результата: позиции пробелов в виде строки, например, `"[5]"` для `"Hello world"`.

Тестовый датасет предоставлен в файле `/kaggle/input/test-3/dataset_1937770_3.txt` в формате CSV с колонками `id` и текстом без пробелов.

## 2. Выбор модели
Для решения выбрана модель `google/byt5-small` из библиотеки Hugging Face `transformers`. Основные причины выбора:
- **Компактный размер**: из-за её компактного размера, что делает её подходящей для работы в Kaggle-ноутбуке с ускорителем GPU T4 x2 (16 ГБ памяти). Модель `google/byt5-base`, имеющая больший размер, не использовалась из-за ограничений по памяти GPU, которые приводили к ошибкам нехватки памяти при загрузке и обучении.
- **Обработка на уровне байтов**: ByT5 работает с текстами как с последовательностями байтов UTF-8, что делает её универсальной для английских и русских текстов без необходимости предопределённого словаря токенов.
- **Поддержка sequence-to-sequence**: Архитектура T5 (`T5ForConditionalGeneration`) подходит для задачи преобразования текста (вход: `"insert spaces: Helloworld"`, выход: `"Hello world"`).

## 3. Этапы решения

### 3.1. Подготовка данных
1. **Генерация обучающего датасета**:
   - Функция `generate_data(12000)` создаёт синтетический датасет из 12,000 примеров, используя случайные комбинации слов из файлов `/kaggle/input/word-segmentation/train.txt` и `/kaggle/input/word-segmentation/valid.txt`.
   - Формат: `{"input": "insert spaces: Helloworld", "output": "Hello world"}`.
   - Датасет разделён на обучающую (10,800 примеров) и валидационную (1,200 примеров) выборки с помощью `train_test_split(test_size=0.1)`.
   - Логирование: Время создания и статистика записаны в `word_segmentation.log`.

2. **Токенизация**:
   - Функция `preprocess` токенизирует входные и выходные тексты с помощью `ByT5Tokenizer`:
     - Вход: `input_ids`, `attention_mask`.
     - Выход: `labels` (токенизированный `output`).
     - Параметры: `max_length=256`, `truncation=True`.
   - Токенизация выполнена с помощью `dataset_dict.map(preprocess, batched=True)` для эффективности.
   - Ненужные колонки (`input`, `output`) удалены из датасета.

3. **Data Collator**:
   - Использован `DataCollatorForSeq2Seq` для автоматического паддинга и форматирования батчей, совместимых с `T5ForConditionalGeneration`.

### 3.2. Обучение модели
1. **Гиперпараметры**:
   - Эпохи: 10.
   - Размер батча: 16 (эффективный батч 32 с накоплением градиентов `GRADIENT_ACCUMULATION_STEPS=2`).
   - Скорость обучения: `2e-5` с оптимизатором AdamW (`weight_decay=0.01`, `eps=1e-8`).
   - Планировщик: `LambdaLR` с разогревом (`WARMUP_STEPS=100`) и линейным затуханием.
   - Логирование: Каждые 50 шагов (`LOG_STEPS`).
   - Валидация: Каждые 200 шагов (`EVAL_STEPS`).

2. **Обучение**:
   - Модель переведена на GPU (`model.to('cuda')`) и в режим обучения (`model.train()`).
   - Цикл обучения включает:
     - Прямой проход через модель для вычисления потерь и `logits`.
     - Накопление градиентов для эффективного батча 32.
     - Ограничение градиентов (`clip_grad_norm_(max_norm=1.0)`).
     - Обновление параметров (`optimizer.step()`) и сброс градиентов.
     - Расчёт точности с помощью функции `compute_accuracy` (сравнение предсказанных токенов с метками).
   - Валидация: Проводится каждые 200 шагов на 100 батчах валидационной выборки, отслеживается лучшая потеря (`best_eval_loss`).

3. **Сохранение модели**:
   - Модель, оптимизатор, планировщик и лучшая валидационная потеря сохранены в `final_model.pt`.
   - Токенизатор сохранён в `./final_model`.

### 3.3. Инференс на тестовом датасете
1. **Чтение тестового датасета**:
   - Файл `/kaggle/input/test-3/dataset_1937770_3.txt` прочитан в DataFrame `df` с колонками `"id"` и `"text_no_spaces"`.
   - Обработаны ошибки формата строк с логированием пропущенных строк.

2. **Пакетное предсказание**:
   - Функция `batch_insert_spaces` выполняет инференс:
     - Загружает токенизатор и модель из `./final_model` и `final_model.pt`.
     - Обрабатывает тексты батчами (`batch_size=8`) с префиксом `"insert spaces: "`.
     - Параметры генерации: `max_length=512`, `num_beams=3`, `early_stopping=True`, `length_penalty=0.8`.
     - Результаты декодируются в тексты с пробелами (например, `"Hello world"`).
   - Логируется время выполнения и среднее время на текст.

3. **Преобразование в позиции пробелов**:
   - Функция `text_to_positions` преобразует предсказанные тексты (`predicted_text`) в позиции пробелов относительно исходных текстов (`text_no_spaces`).
   - Результаты добавлены в `df` как колонка `"predicted_positions"` в формате строки (например, `"[5]"`).

4. **Создание итогового DataFrame**:
   - Создан `df_n` с колонками `"id"` и `"predicted_positions"`.
   - Сохранён в `task_data_with_positions.csv` с кодировкой UTF-8.

### 3.4. Логирование
- Все этапы сопровождаются логированием в файл `word_segmentation.log` и консоль:
  - Генерация данных: Время и статистика датасета.
  - Загрузка модели: Время загрузки `google/byt5-small`.
  - Токенизация: Время обработки датасета.
  - Обучение: Потери, точность, скорость обучения каждые 50 шагов; валидационные метрики каждые 200 шагов.
  - Инференс: Время предсказания и среднее время на текст.
  - Сохранение: Подтверждение создания выходного файла.

## 4. Технические особенности
- **Kaggle и GPU**:
  - Использован ускоритель GPU T4 x2 (16 ГБ) для обучения и инференса.
  - Проверено наличие GPU: `torch.cuda.is_available()` и `torch.cuda.get_device_name(0)`.
- **Зависимости**:
  ```bash
  !pip install transformers datasets torch pandas

- **Воспроизводимость**:
  - Установлены `random.seed(42)` и `torch.manual_seed(42)` для воспроизводимости генерации данных и обучения.
- **Обработка ошибок**:
  - Проверки на некорректные строки в тестовом датасете.
  - Обработка исключений в функциях токенизации и инференса.
- **Оптимизация памяти**:
  - Накопление градиентов (`GRADIENT_ACCUMULATION_STEPS=2`) для эффективного батча 32.
  - Возможность использования mixed precision для снижения потребления памяти GPU (не реализовано, но рекомендовано).

## 5. Результаты
- **Обучающий датасет**: 10,800 обучающих и 1,200 валидационных примеров.
- **Тестовый датасет**: Обработан файл `dataset_1937770_3.txt`, результаты сохранены в `task_data_with_positions.csv` с колонками `id` и `predicted_positions`.
- **Качество**: Точность и потери отслеживались во время обучения, лучшая валидационная потеря сохранена.

## 7. Итог
Подход включает полный цикл решения: от генерации синтетического датасета до предсказания и сохранения позиций пробелов. Использование `google/byt5-small` позволило эффективно работать в условиях ограниченной памяти GPU T4 x2, а подробное логирование упростило отладку и контроль. Результаты сохранены в `task_data_with_positions.csv`, готовом для отправки в Kaggle.
